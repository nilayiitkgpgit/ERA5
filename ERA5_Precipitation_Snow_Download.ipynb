{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7234570",
   "metadata": {},
   "source": [
    "# Building a pipeline for ERA5 data download=>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98315b",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 2 â€” User controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4e64c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR  = 2023\n",
    "START_MONTH = 1\n",
    "\n",
    "END_YEAR    = 2023\n",
    "END_MONTH   = 3\n",
    "\n",
    "BASE_DIR = r\"D:\\PHD IIT KGP\\Codes @ VSCODE\\ERA5 DATA Download\"   # change only this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122650da",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 3 â€” Resume / crash-recovery logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6ad70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_log(logfile):\n",
    "    done = set()\n",
    "    if os.path.exists(logfile):\n",
    "        with open(logfile, \"r\") as f:\n",
    "            for line in f:\n",
    "                done.add(line.strip().split()[0])\n",
    "    return done\n",
    "\n",
    "def write_log(logfile, ym):\n",
    "    with open(logfile, \"a\") as f:\n",
    "        f.write(f\"{ym} DONE\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c766c2",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 4 â€” Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50fa578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdsapi in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7.7)\n",
      "Requirement already satisfied: xarray in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2025.12.0)\n",
      "Requirement already satisfied: netcdf4 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: ecmwf-datastores-client>=0.4.0 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cdsapi) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cdsapi) (2.32.5)\n",
      "Requirement already satisfied: packaging>=24.1 in c:\\users\\nimta\\appdata\\roaming\\python\\python311\\site-packages (from xarray) (25.0)\n",
      "Requirement already satisfied: pandas>=2.2 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xarray) (2.3.3)\n",
      "Requirement already satisfied: cftime in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from netcdf4) (1.6.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from netcdf4) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\nimta\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: attrs in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (25.4.0)\n",
      "Requirement already satisfied: multiurl>=0.3.7 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nimta\\appdata\\roaming\\python\\python311\\site-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (4.15.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nimta\\appdata\\roaming\\python\\python311\\site-packages (from multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.2->xarray) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nimta\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil->multiurl>=0.3.7->ecmwf-datastores-client>=0.4.0->cdsapi) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nimta\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.5.0->cdsapi) (2.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cdsapi xarray netcdf4 tqdm numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6060a9",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 5 â€” ERA5 download logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f8db7",
   "metadata": {},
   "source": [
    "### 5.1 Month iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af94cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_range(start_year, start_month, end_year, end_month):\n",
    "    months = []\n",
    "    for y in range(start_year, end_year + 1):\n",
    "        for m in range(1, 13):\n",
    "            if (y == start_year and m < start_month):\n",
    "                continue\n",
    "            if (y == end_year and m > end_month):\n",
    "                continue\n",
    "            months.append((y, m))\n",
    "    return months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a1afa",
   "metadata": {},
   "source": [
    "### 5.2 ERA5 monthly download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5419f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import calendar\n",
    "\n",
    "def download_month(year, month, out_file):\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    days = [f\"{d:02d}\" for d in range(1, calendar.monthrange(year, month)[1] + 1)]\n",
    "    hours = [f\"{h:02d}:00\" for h in range(24)]\n",
    "\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\n",
    "                \"total_precipitation\",\n",
    "                \"snowfall\",\n",
    "            ],\n",
    "            \"year\": str(year),\n",
    "            \"month\": f\"{month:02d}\",\n",
    "            \"day\": days,\n",
    "            \"time\": hours,\n",
    "            \"format\": \"netcdf\",\n",
    "        },\n",
    "        out_file\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a55071",
   "metadata": {},
   "source": [
    "### 5.3 ZIP detection & extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6ae20",
   "metadata": {},
   "source": [
    "âœ” CRITICAL FIX APPLIED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa7b39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def handle_possible_zip(nc_path, extract_dir):\n",
    "    if zipfile.is_zipfile(nc_path):\n",
    "        print(\"âš  ZIP archive detected (disguised as .nc). Extracting...\")\n",
    "        with zipfile.ZipFile(nc_path, 'r') as z:\n",
    "            z.extractall(extract_dir)\n",
    "        os.remove(nc_path)\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec77b39",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 6 â€” Progress & file size reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "266df6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_gb(filepath):\n",
    "    return os.path.getsize(filepath) / (1024**3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b09aa7",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 7 â€” Hourly â†’ daily accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738193d",
   "metadata": {},
   "source": [
    "### âœ… CORRECTED CHUNK 7 (FINAL VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e55b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ CHUNK 7 â€” Hourly â†’ Daily Accumulation (ERA5-safe)\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "def hourly_to_daily(infile, outfile):\n",
    "    ds = xr.open_dataset(infile)\n",
    "\n",
    "    # ---- FIX 1: normalize time coordinate ----\n",
    "    if \"time\" not in ds.coords:\n",
    "        if \"valid_time\" in ds.coords:\n",
    "            ds = ds.rename({\"valid_time\": \"time\"})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No recognizable time coordinate (time/valid_time) found in dataset.\"\n",
    "            )\n",
    "\n",
    "    # ---- FIX 2: daily accumulation ----\n",
    "    ds_daily = ds.resample(time=\"1D\").sum()\n",
    "\n",
    "    # ---- FIX 3: ERA5 variable names & units ----\n",
    "    if \"tp\" in ds_daily:\n",
    "        ds_daily[\"tp\"] *= 1000\n",
    "        ds_daily[\"tp\"].attrs[\"units\"] = \"mm/day\"\n",
    "        ds_daily[\"tp\"].attrs[\"long_name\"] = \"Daily total precipitation\"\n",
    "\n",
    "    if \"sf\" in ds_daily:\n",
    "        ds_daily[\"sf\"] *= 1000\n",
    "        ds_daily[\"sf\"].attrs[\"units\"] = \"mm/day\"\n",
    "        ds_daily[\"sf\"].attrs[\"long_name\"] = \"Daily snowfall\"\n",
    "\n",
    "    ds_daily.to_netcdf(outfile)\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa884d9",
   "metadata": {},
   "source": [
    "## ðŸ”¹ CHUNK 8 â€” Main driver script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415fffa",
   "metadata": {},
   "source": [
    "### âœ… CHUNK 8A â€” Disk growth monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58f9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ CHUNK 8A â€” SAFE file verification & size reporting (Windows-safe)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "def verify_download(filepath, wait=5):\n",
    "    \"\"\"\n",
    "    Waits briefly after CDS finishes and verifies\n",
    "    file existence and final size.\n",
    "    \"\"\"\n",
    "    time.sleep(wait)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(\"Download finished but file not found.\")\n",
    "\n",
    "    size_gb = os.path.getsize(filepath) / (1024 ** 3)\n",
    "    print(f\"ðŸ“¦ Download verified. File size: {size_gb:.2f} GB\")\n",
    "\n",
    "    return size_gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415f291",
   "metadata": {},
   "source": [
    "### ðŸ”¹ UPDATED CHUNK 8 â€” Main driver (with monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f9b9b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERA5 Monthly Download:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¬‡ï¸ Downloading 2023-01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 12:11:40,881 INFO [2025-12-03T00:00:00Z] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-12-17 12:11:41,757 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2025-12-17 12:11:41,760 INFO Request ID is 0b5878bb-d81a-47c2-83ca-2b0133a5c254\n",
      "2025-12-17 12:11:41,978 INFO status has been updated to accepted\n",
      "2025-12-17 12:11:51,053 INFO status has been updated to successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Download verified. File size: 0.92 GB\n",
      "ðŸ§® Converting to daily accumulated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERA5 Monthly Download:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [11:29<22:59, 689.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¬‡ï¸ Downloading 2023-02 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 12:23:10,766 INFO [2025-12-03T00:00:00Z] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-12-17 12:23:11,501 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2025-12-17 12:23:11,501 INFO Request ID is ca8ed084-c87d-4311-9349-23ce146993ce\n",
      "2025-12-17 12:23:11,725 INFO status has been updated to accepted\n",
      "2025-12-17 12:23:26,274 INFO status has been updated to running\n",
      "2025-12-17 12:27:33,876 INFO status has been updated to successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Download verified. File size: 0.83 GB\n",
      "ðŸ§® Converting to daily accumulated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERA5 Monthly Download:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [21:27<10:35, 635.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â¬‡ï¸ Downloading 2023-03 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 12:33:08,519 INFO [2025-12-03T00:00:00Z] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-12-17 12:33:09,400 INFO [2025-12-11T00:00:00] Please note that a dedicated catalogue entry for this dataset, post-processed and stored in Analysis Ready Cloud Optimized (ARCO) format (Zarr), is available for optimised time-series retrievals (i.e. for retrieving data from selected variables for a single point over an extended period of time in an efficient way). You can discover it [here](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)\n",
      "2025-12-17 12:33:09,400 INFO Request ID is 11653a3b-4fa0-4269-94f6-a65cffd327ce\n",
      "2025-12-17 12:33:09,639 INFO status has been updated to accepted\n",
      "2025-12-17 12:33:23,977 INFO status has been updated to running\n",
      "2025-12-17 12:37:32,008 INFO status has been updated to successful\n",
      "Recovering from connection error [('Connection broken: IncompleteRead(421578496 bytes read, 558827916 more expected)', IncompleteRead(421578496 bytes read, 558827916 more expected))], attempt 1 of 500\n",
      "Retrying in 120 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Download verified. File size: 0.91 GB\n",
      "ðŸ§® Converting to daily accumulated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERA5 Monthly Download: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [32:19<00:00, 646.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ CHUNK 8 â€” Main driver script (Windows-safe, ZIP-safe)\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DIR   = os.path.join(BASE_DIR, \"raw_hourly\")\n",
    "DAILY_DIR = os.path.join(BASE_DIR, \"daily_accumulated\")\n",
    "LOG_FILE  = os.path.join(BASE_DIR, \"download_log.txt\")\n",
    "\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(DAILY_DIR, exist_ok=True)\n",
    "\n",
    "done_months = read_log(LOG_FILE)\n",
    "months = month_range(START_YEAR, START_MONTH, END_YEAR, END_MONTH)\n",
    "\n",
    "for year, month in tqdm(months, desc=\"ERA5 Monthly Download\"):\n",
    "    ym = f\"{year}-{month:02d}\"\n",
    "    if ym in done_months:\n",
    "        continue\n",
    "\n",
    "    raw_file = os.path.join(RAW_DIR, f\"tp_sf_{year}_{month:02d}.nc\")\n",
    "\n",
    "    print(f\"\\nâ¬‡ï¸ Downloading {ym} ...\")\n",
    "    download_month(year, month, raw_file)\n",
    "\n",
    "    # ---- SAFE verification (NO file locking) ----\n",
    "    verify_download(raw_file)\n",
    "\n",
    "    # ---- ZIP-IN-DISGUISE HANDLING ----\n",
    "    was_zip = handle_possible_zip(raw_file, RAW_DIR)\n",
    "    if was_zip:\n",
    "        raw_file = os.path.join(\n",
    "            RAW_DIR, \"data_stream-oper_stepType-accum.nc\"\n",
    "        )\n",
    "\n",
    "    daily_file = os.path.join(\n",
    "        DAILY_DIR, f\"ERA5_DAILY_TP_SF_{year}_{month:02d}.nc\"\n",
    "    )\n",
    "\n",
    "    print(\"ðŸ§® Converting to daily accumulated...\")\n",
    "    hourly_to_daily(raw_file, daily_file)\n",
    "\n",
    "    write_log(LOG_FILE, ym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1593c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9567fa74",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
